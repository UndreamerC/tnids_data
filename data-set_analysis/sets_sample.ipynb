{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9446d86-0dbc-489c-867d-bc77d6438d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, input_dir='sample_data', output_dir='sample_data/output'):\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "    \n",
    "    def load_data(self, filename, column_names=None):\n",
    "        filepath = os.path.join(self.input_dir, filename)\n",
    "        try:\n",
    "            data = pd.read_csv(filepath, header=0,low_memory=False)\n",
    "            if column_names:\n",
    "                data.columns = column_names\n",
    "            return data\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File '{filename}' not found in '{self.input_dir}'.\")\n",
    "            return None\n",
    "\n",
    "    def save_data(self, data, filename):\n",
    "        output_path = os.path.join(self.output_dir, filename)\n",
    "        data.to_csv(output_path, index=False)\n",
    "        print(f\"Data saved to {output_path}\")\n",
    "\n",
    "    def compare_csv(self, file1, file2):\n",
    "        try:\n",
    "            df1 = pd.read_csv(file1)\n",
    "            df2 = pd.read_csv(file2)\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error loading files: {e}\")\n",
    "            return\n",
    "\n",
    "        print(f\"{file1} shape: {df1.shape}\")\n",
    "        print(f\"{file2} shape: {df2.shape}\")\n",
    "\n",
    "        if df1.shape != df2.shape:\n",
    "            print(\"Files have different shapes.\")\n",
    "            return\n",
    "\n",
    "        if df1.equals(df2):\n",
    "            print(\"The two CSV files are identical.\")\n",
    "        else:\n",
    "            differences = df1.compare(df2)\n",
    "            print(\"Differences found between files:\")\n",
    "            print(differences)\n",
    "\n",
    "    def combine_data_with_labels(self, data_file, label_file, output_file):\n",
    "        data = self.load_data(data_file)\n",
    "        labels = self.load_data(label_file)\n",
    "        \n",
    "        if data is None or labels is None:\n",
    "            print(\"Failed to load data or labels.\")\n",
    "            return\n",
    "\n",
    "        if len(data) != len(labels):\n",
    "            print(\"Data and labels have different lengths...\")\n",
    "            print(len(data),len(labels))\n",
    "            \n",
    "            return\n",
    "\n",
    "        combined = pd.concat([data, labels[['x']]], axis=1)\n",
    "        self.save_data(combined, output_file)\n",
    "        print(f\"Data and labels combined and saved as '{output_file}'\")\n",
    "\n",
    "    def filter_and_save_data(self, input_file, output_train_file, output_test_file, \n",
    "                         rows_to_select=20000, target_ratio=0.5, test_size=0.2):\n",
    "        # Read the dataset\n",
    "        data = pd.read_csv(input_file)\n",
    "        \n",
    "        # Separate the data into 'normal' and 'malicious' samples\n",
    "        normal_data = data[data['x'] == 0]\n",
    "        malicious_data = data[data['x'] == 1]\n",
    "        \n",
    "        # Calculate required number of samples for each type\n",
    "        normal_count = int(rows_to_select * (1 - target_ratio))\n",
    "        malicious_count = rows_to_select - normal_count  # Remaining rows for 'x=1'\n",
    "        \n",
    "        # Check if there are enough samples to meet the target counts\n",
    "        if len(normal_data) < normal_count:\n",
    "            print(f\"Warning: Not enough normal samples to reach {normal_count}. Adjusting to {len(normal_data)}.\")\n",
    "            normal_count = len(normal_data)\n",
    "            malicious_count = rows_to_select - normal_count\n",
    "        \n",
    "        if len(malicious_data) < malicious_count:\n",
    "            print(f\"Warning: Not enough malicious samples to reach {malicious_count}. Adjusting to {len(malicious_data)}.\")\n",
    "            malicious_count = len(malicious_data)\n",
    "            normal_count = rows_to_select - malicious_count\n",
    "        \n",
    "        # Sample data from each group\n",
    "        sampled_normal = normal_data.sample(n=normal_count, random_state=42)\n",
    "        sampled_malicious = malicious_data.sample(n=malicious_count, random_state=42)\n",
    "        \n",
    "        # Combine and shuffle the sampled data\n",
    "        filtered_data = pd.concat([sampled_normal, sampled_malicious]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        # Split the filtered data into training and testing sets\n",
    "        train_data, test_data = train_test_split(filtered_data, test_size=test_size, random_state=42)\n",
    "        \n",
    "        # Count the distribution of 'x=0' and 'x=1' in each set\n",
    "        train_count_0 = (train_data['x'] == 0).sum()\n",
    "        train_count_1 = (train_data['x'] == 1).sum()\n",
    "        test_count_0 = (test_data['x'] == 0).sum()\n",
    "        test_count_1 = (test_data['x'] == 1).sum()\n",
    "        \n",
    "        print(f\"Training set: x=0 count: {train_count_0}, x=1 count: {train_count_1}\")\n",
    "        print(f\"Testing set: x=0 count: {test_count_0}, x=1 count: {test_count_1}\")\n",
    "        \n",
    "        # Save the training and testing data to their respective output files\n",
    "        self.save_data(train_data, output_train_file)\n",
    "        self.save_data(test_data, output_test_file)\n",
    "        print(f\"Training data saved to '{output_train_file}', Testing data saved to '{output_test_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17508a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用示例\n",
    "processor = DataProcessor(input_dir='sample_data', output_dir='sample_data/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb31a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to sample_data/output/Fuzzing_data.csv\n",
      "Data and labels combined and saved as 'Fuzzing_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# 步骤3: 合并数据与标签文件\n",
    "#targrt='Mirai'\n",
    "targrt='Fuzzing'\n",
    "processor.combine_data_with_labels(f'output/{targrt}_pcap_context.csv', f'{targrt}_labels.csv', f'{targrt}_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "037c3e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: x=0 count: 10471, x=1 count: 10529\n",
      "Testing set: x=0 count: 4529, x=1 count: 4471\n",
      "Data saved to sample_data/output/Fuzzing_data1.csv\n",
      "Data saved to sample_data/output/Fuzzing_data2.csv\n",
      "Training data saved to 'Fuzzing_data1.csv', Testing data saved to 'Fuzzing_data2.csv'\n"
     ]
    }
   ],
   "source": [
    "# 步骤4: 筛选特定行并保存\n",
    "processor.filter_and_save_data(f'sample_data/output/{targrt}_data.csv', f'{targrt}_data1.csv',f'{targrt}_data2.csv',rows_to_select=30000, target_ratio=0.5, test_size=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nids",
   "language": "python",
   "name": "nids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
