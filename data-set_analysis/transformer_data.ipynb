{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02b9c8b5-1b82-4479-b8be-d0858480daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "import platform\n",
    "import glob\n",
    "import time\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import time\n",
    "import psutil\n",
    "import csv\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import logging\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfff23af-c0e5-4179-9235-05bb2715dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_memory = 0\n",
    "\n",
    "def print_memory_usage():\n",
    "    global max_memory\n",
    "    process = psutil.Process(os.getpid())  # Get the current process\n",
    "    memory_info = process.memory_info()  # Get memory usage information\n",
    "    current_memory = memory_info.rss / 1024 ** 2  # Convert memory usage to MB\n",
    "    if current_memory > max_memory:\n",
    "        max_memory = current_memory  # Update max memory usage\n",
    "        print(f\"memory_usage: {current_memory:.2f} MB\")  # Print current memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a76a1f06-12fa-4770-a15e-253f17ddaed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class PrepPcap:\n",
    "    def __init__(self, file_path, output_path):\n",
    "        self.path = Path(file_path)\n",
    "        self.outdir = Path(output_path)\n",
    "        self.tsvpath = self.outdir / self.path.with_suffix('.tsv').name\n",
    "        self.tsv_time = None\n",
    "        self.parse_type = None\n",
    "        self._tshark = self._get_tshark_path()\n",
    "        \n",
    "        # Prepare the environment\n",
    "        self.__prep__()\n",
    "\n",
    "    # Fields to be parsed, defined as a class attribute\n",
    "    _FIELDS = [\n",
    "        \"-e\", \"frame.time_epoch\", \"-e\", \"frame.len\", \"-e\", \"eth.src\", \"-e\", \"eth.dst\",\n",
    "        \"-e\", \"ip.src\", \"-e\", \"ip.dst\", \"-e\", \"ip.len\", \"-e\", \"tcp.srcport\", \"-e\", \"tcp.dstport\",\n",
    "        \"-e\", \"udp.srcport\", \"-e\", \"udp.dstport\", \"-e\", \"arp.opcode\", \"-e\", \"arp.src.hw_mac\",\n",
    "        \"-e\", \"arp.src.proto_ipv4\", \"-e\", \"arp.dst.hw_mac\", \"-e\", \"arp.dst.proto_ipv4\",\n",
    "        \"-e\", \"icmp.type\", \"-e\", \"icmp.code\", \"-e\", \"ipv6.src\", \"-e\", \"ipv6.dst\"\n",
    "    ]\n",
    "\n",
    "    def pcap2tsv_with_tshark(self):\n",
    "        logging.info('Parsing with tshark...')\n",
    "        start_time = time.time()\n",
    "        cmd = [str(self._tshark), \"-r\", str(self.path), \"-T\", \"fields\"] + self._FIELDS + [\"-E\", \"header=y\", \"-E\", \"occurrence=f\"]\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        self.outdir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Run tshark command\n",
    "        try:\n",
    "            with open(self.tsvpath, 'w') as output_file:\n",
    "                subprocess.run(cmd, stdout=output_file, stderr=subprocess.PIPE, check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            logging.error(\"Error occurred while executing tshark command:\")\n",
    "            logging.error(e.stderr.decode())\n",
    "            raise\n",
    "        \n",
    "        end_time = time.time()\n",
    "        self.tsv_time = end_time - start_time\n",
    "        logging.info(f\"tshark parsing complete. File saved as: {self.tsvpath}\")\n",
    "        \n",
    "    def _get_tshark_path(self):\n",
    "        if platform.system() == 'Windows':\n",
    "            default_path = Path(r'C:\\Program Files\\Wireshark\\tshark.exe')\n",
    "            if default_path.is_file():\n",
    "                return default_path\n",
    "        for path in os.environ['PATH'].split(os.pathsep):\n",
    "            tshark_path = Path(path) / 'tshark'\n",
    "            if tshark_path.is_file():\n",
    "                return tshark_path\n",
    "        raise FileNotFoundError(\"tshark not found in PATH. Please ensure it is installed.\")\n",
    "\n",
    "    def __prep__(self):\n",
    "        if not self.path.is_file():\n",
    "            raise FileNotFoundError(f\"File: {self.path} does not exist\")\n",
    "        \n",
    "        file_type = self.path.suffix.lower()\n",
    "        if file_type == '.tsv':\n",
    "            self.parse_type = \"tsv\"\n",
    "        elif file_type in {'.pcap', '.pcapng'}:\n",
    "            if not self.tsvpath.is_file():\n",
    "                self.pcap2tsv_with_tshark()\n",
    "            else:\n",
    "                logging.info(f\"{self.tsvpath} already exists!\")\n",
    "            self.parse_type = \"tsv\"\n",
    "        else:\n",
    "            raise ValueError(f\"File: {self.path} is not a tsv or pcap file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c255b813-e6e9-4c0c-a13e-b7e8074366a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(df, output_file, verbose=True):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        df.to_csv(output_file, index=False)\n",
    "        if verbose:\n",
    "            logging.info(f\"Result saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving file {output_file}: {type(e).__name__} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7116bd54-d94f-49dc-bc72-92bff168db52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subdirectories(directory, recursive):\n",
    "    \"\"\"Retrieve subdirectories based on recursion flag.\"\"\"\n",
    "    if recursive:\n",
    "        return [os.path.join(directory, d) for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    return [directory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "744b8ed1-c97d-40b0-8c4a-dc06054e63d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tsv_file, output_file='output.csv', save_interval=100000, chunksize=50000):\n",
    "    start_time = time.time()\n",
    "    print(\"Reading file\")\n",
    "\n",
    "    # Initialize flow_stats dictionary and list for temporary storage\n",
    "    flow_stats = {}\n",
    "    flow_stats_list = []\n",
    "\n",
    "    # Open output file for writing\n",
    "    with open(output_file, mode='w', newline='') as f:\n",
    "        writer = None\n",
    "\n",
    "        # Read file in chunks\n",
    "        for chunk in pd.read_csv(tsv_file, sep='\\t', low_memory=False, chunksize=chunksize):\n",
    "            for i, row in chunk.iterrows():\n",
    "                frame_time = row['frame.time_epoch']\n",
    "                src_ip, dst_ip = '', ''\n",
    "\n",
    "                # Identify IP and protocol information\n",
    "                if pd.notna(row.get('ip.src')):\n",
    "                    src_ip, dst_ip = row['ip.src'], row['ip.dst']\n",
    "                elif pd.notna(row.get('ipv6.src')):\n",
    "                    src_ip, dst_ip = row['ipv6.src'], row['ipv6.dst']\n",
    "                elif pd.notna(row.get('arp.src.proto_ipv4')):\n",
    "                    src_ip, dst_ip = row['arp.src.proto_ipv4'], row['arp.dst.proto_ipv4']\n",
    "                    #protocol = 'ARP'\n",
    "\n",
    "                src_eth, dst_eth = row['eth.src'], row['eth.dst']\n",
    "\n",
    "                # Determine protocol type if not ARP\n",
    "                flow_key = (src_ip, dst_ip, src_eth, dst_eth) #####different from transformer\n",
    "                \n",
    "\n",
    "                ##########################################################################################\n",
    "                # Initialize or update flow statistics\n",
    "                if flow_key not in flow_stats:\n",
    "                    flow_stats[flow_key] = {\n",
    "                        'Src_ip': src_ip,  #index\n",
    "                        'Dst_ip': dst_ip,\n",
    "                        'Src_eth': src_eth, \n",
    "                        'Dst_eth': dst_eth,\n",
    "                        'frame_len':0,\n",
    "                        'TCP':0,\n",
    "                        'UDP':0,\n",
    "                        'ICMP':0,\n",
    "                        'Total': 0,\n",
    "                        'last_time': frame_time, \n",
    "                        'Frequency':0\n",
    "                    }\n",
    "\n",
    "                flow_stats[flow_key]['frame_len'] = row['frame.len']\n",
    "                flow_stats[flow_key]['TCP'] = row['tcp.dstport']\n",
    "                flow_stats[flow_key]['UDP'] = row['udp.dstport']\n",
    "                flow_stats[flow_key]['ICMP'] = row['icmp.type']\n",
    "                #flow_stats[flow_key]['ARP'] = row['arp.opcode']\n",
    "                #flow_stats[flow_key][protocol] += 1\n",
    "\n",
    "\n",
    "                # Calculate Frequency for the current flow\n",
    "                time_interval = float(frame_time) - float(flow_stats[flow_key]['last_time'])\n",
    "                frequency = flow_stats[flow_key]['Total'] / time_interval if time_interval > 0 else 1\n",
    "                Lambda=2\n",
    "                factor = math.pow(2, (-Lambda * time_interval))\n",
    "                flow_stats[flow_key]['Total'] = flow_stats[flow_key]['Total']*factor+1\n",
    "                \n",
    "                flow_stats[flow_key]['Frequency'] = frequency\n",
    "                flow_stats[flow_key]['last_time'] = frame_time\n",
    "                ##########################################################################################\n",
    "\n",
    "\n",
    "                # Create a snapshot of the current flow stats\n",
    "                current_stats = flow_stats[flow_key].copy()\n",
    "                flow_stats_list.append(current_stats)\n",
    "\n",
    "                # Write batch to file every save_interval rows\n",
    "                if len(flow_stats_list) >= save_interval:\n",
    "                    df_batch = pd.DataFrame(flow_stats_list)\n",
    "                    flow_stats_list.clear()  # Clear the temporary storage\n",
    "\n",
    "                    # Only write to CSV if the batch is not empty\n",
    "                    if not df_batch.empty:\n",
    "                        if writer is None:\n",
    "                            # Initialize DictWriter and write header once\n",
    "                            writer = csv.DictWriter(f, fieldnames=df_batch.columns)\n",
    "                            writer.writeheader()\n",
    "                        writer.writerows(df_batch.to_dict(orient='records'))\n",
    "                        print_memory_usage()\n",
    "\n",
    "        # Write any remaining data\n",
    "        if flow_stats_list:\n",
    "            df_batch = pd.DataFrame(flow_stats_list)\n",
    "            if writer is None:\n",
    "                writer = csv.DictWriter(f, fieldnames=df_batch.columns)\n",
    "                writer.writeheader()\n",
    "            writer.writerows(df_batch.to_dict(orient='records'))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    analysis_time = end_time - start_time\n",
    "    print(f\"Data saved to {output_file}. Processed in {analysis_time:.2f} seconds.\")\n",
    "    \n",
    "    return analysis_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "672e391f-c8cc-4a10-9836-5e462f46df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk, prev_chunk, next_chunk, n,select_n=5):\n",
    "    data = pd.concat([prev_chunk, chunk, next_chunk]).reset_index(drop=True)\n",
    "    data_len = len(data)\n",
    "    chunk_len = len(chunk)\n",
    "\n",
    "\n",
    "    cols = data.columns\n",
    "    new_columns = (\n",
    "        [f\"backward_{i+1}_{col}\" for i in range(select_n) for col in cols] +\n",
    "        [f\"current_{col}\" for col in cols] +\n",
    "        [f\"forward_{i+1}_{col}\" for i in range(select_n) for col in cols]\n",
    "    )\n",
    "\n",
    "    context_data = []\n",
    "    m = len(prev_chunk)\n",
    "\n",
    "    for index in range(m, m + chunk_len):\n",
    "        row = data.iloc[index]\n",
    "        current_src_eth = row['Src_eth']\n",
    "\n",
    "        backward_context = [dict.fromkeys(data.columns, None) for _ in range(select_n)]\n",
    "        forward_context = [dict.fromkeys(data.columns, None) for _ in range(select_n)]\n",
    "\n",
    "        back_count = 0\n",
    "        for i in range(1, n + 1):\n",
    "            if index - i < 0:\n",
    "                break\n",
    "            if data.iloc[index - i]['Src_eth'] == current_src_eth:\n",
    "                backward_context[4 - back_count] = data.iloc[index - i].to_dict()\n",
    "                back_count += 1\n",
    "                if back_count >= select_n:\n",
    "                    break\n",
    "\n",
    "        forward_count = 0\n",
    "        for i in range(1, n + 1):\n",
    "            if index + i >= data_len:\n",
    "                break\n",
    "            if data.iloc[index + i]['Src_eth'] == current_src_eth:\n",
    "                forward_context[forward_count] = data.iloc[index + i].to_dict()\n",
    "                forward_count += 1\n",
    "                if forward_count >= select_n:\n",
    "                    break\n",
    "\n",
    "        combined_context = []\n",
    "        for b_context in backward_context:\n",
    "            combined_context.extend(b_context.values())\n",
    "        combined_context.extend(row.values)\n",
    "        for f_context in forward_context:\n",
    "            combined_context.extend(f_context.values())\n",
    "\n",
    "        context_data.append(combined_context)\n",
    "\n",
    "    return pd.DataFrame(context_data, columns=new_columns)\n",
    "\n",
    "def save_to_csv_partial(df, output_file, mode='a'):\n",
    "    if os.path.exists(output_file):\n",
    "        df.to_csv(output_file, mode=mode, header=False , index=False)\n",
    "    else:\n",
    "        df.to_csv(output_file, mode='w', header=True , index=False)\n",
    "\n",
    "\n",
    "def extract_context_parallel(file_path, output_file, chunk_size=100000, n=10):\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "        print(f\"Delete {output_file}\")\n",
    "\n",
    "    prev_chunk = pd.DataFrame()  \n",
    "    with pd.read_csv(file_path, chunksize=chunk_size) as reader:\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            current_chunk = next(reader, None)  \n",
    "            \n",
    "            while current_chunk is not None:\n",
    "                try:\n",
    "                    next_chunk = next(reader, None) \n",
    "                except StopIteration:\n",
    "                    next_chunk = None \n",
    "\n",
    "                if next_chunk is not None:\n",
    "                    next_chunk_context = next_chunk.iloc[:n]\n",
    "                else:\n",
    "                    next_chunk_context = pd.DataFrame() \n",
    "\n",
    "                futures.append(executor.submit(process_chunk, current_chunk, prev_chunk, next_chunk_context, n))\n",
    "\n",
    "                prev_chunk = current_chunk.tail(n)\n",
    "                current_chunk = next_chunk  \n",
    "\n",
    "\n",
    "            for future in futures:\n",
    "                while not future.done(): \n",
    "                    time.sleep(0.01)  \n",
    "                df_chunk = future.result()  \n",
    "                save_to_csv_partial(df_chunk, output_file, mode='a')\n",
    "                del df_chunk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df504bc1-fc5f-4c39-aeab-150baf35400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pcap_files(pcap_files, outdir, debug):\n",
    "    \"\"\"Process each PCAP file in the list.\"\"\"\n",
    "    if not debug:\n",
    "        for pcap_path in pcap_files:\n",
    "            print(f\"\\nStart {pcap_path}\")\n",
    "            PP = PrepPcap(pcap_path, outdir)\n",
    "            print_memory_usage()\n",
    "            name = os.path.splitext(os.path.basename(pcap_path))[0]\n",
    "            feature_csv = os.path.join(outdir, f'{name}_feature.csv')\n",
    "            context_csv = os.path.join(outdir, f'{name}_context.csv')\n",
    "            PP.analysis_time = extract_features(PP.tsvpath,feature_csv)\n",
    "            print(\"***************\")\n",
    "            extract_context_parallel(feature_csv,context_csv,100000,10)\n",
    "            print_memory_usage()\n",
    "            print(f'Process {pcap_path} finished!') \n",
    "    else:\n",
    "        print(\"Debug mode enabled. Skipping detailed processing.\")\n",
    "        s=time.time()\n",
    "        for pcap_path in pcap_files:\n",
    "            print(f\"\\nStart {pcap_path}\")\n",
    "            PP = PrepPcap(pcap_path, outdir)\n",
    "            print_memory_usage()\n",
    "            name = os.path.splitext(os.path.basename(pcap_path))[0]\n",
    "            feature_csv = os.path.join(outdir, f'{name}_feature.csv')\n",
    "            context_csv = os.path.join(outdir, f'{name}_context.csv')\n",
    "            #PP.analysis_time = extract_features(PP.tsvpath,feature_csv)\n",
    "            print(\"***************\")\n",
    "            extract_context_parallel(feature_csv,context_csv,100000,10)\n",
    "            print_memory_usage()\n",
    "        e=time.time()\n",
    "        print(e-s)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "119e270e-a7ae-40f1-8bca-63df77f0a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser():\n",
    "    parser = argparse.ArgumentParser(description=\"Process PCAP files in a directory.\")\n",
    "    \n",
    "    # Main directory argument\n",
    "    parser.add_argument(\n",
    "        '-d', '--directory', \n",
    "        required=True, \n",
    "        help=\"Specify the directory containing PCAP files.\"\n",
    "    )\n",
    "    \n",
    "    # Analysis options\n",
    "    parser.add_argument(\n",
    "        '-a', '--analysis', \n",
    "        action='store_true', \n",
    "        help=\"Generate a summary analysis of the PCAP files.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-r', '--recursive', \n",
    "        action='store_true', \n",
    "        help=\"Recursively traverse subdirectories for PCAP files.\"\n",
    "    )\n",
    "    \n",
    "    # Debugging options\n",
    "    parser.add_argument(\n",
    "        '-b', '--debug', \n",
    "        action='store_true', \n",
    "        help=\"Enable debug mode for additional output.\"\n",
    "    )\n",
    "\n",
    "    # Parse the arguments from the command line\n",
    "    #args = parser.parse_args()  \n",
    "    args = parser.parse_args(['-d', 'Data/kitsune','-a','-b'])  \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "355fca53-edbf-4ed7-b734-3e9e6eca7c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 16:49:45,744 - INFO - Data/kitsune/output/Video_Injection_pcap.tsv already exists!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis for directory 'Data/kitsune'\n",
      "Debug mode enabled. Skipping detailed processing.\n",
      "\n",
      "Start Data/kitsune/Video_Injection_pcap.pcapng\n",
      "memory_usage: 259.89 MB\n",
      "***************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 16:54:30,797 - INFO - Data/kitsune/output/OS_Scan_pcap.tsv already exists!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_usage: 320.86 MB\n",
      "\n",
      "Start Data/kitsune/OS_Scan_pcap.pcapng\n",
      "***************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 16:58:21,888 - INFO - Data/kitsune/output/Fuzzing_pcap.tsv already exists!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_usage: 336.39 MB\n",
      "\n",
      "Start Data/kitsune/Fuzzing_pcap.pcapng\n",
      "***************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 17:03:00,100 - INFO - Data/kitsune/output/Active_Wiretap_pcap.tsv already exists!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Data/kitsune/Active_Wiretap_pcap.pcapng\n",
      "***************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 17:07:40,767 - INFO - Data/kitsune/output/SSDP_Flood_pcap.tsv already exists!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_usage: 358.27 MB\n",
      "\n",
      "Start Data/kitsune/SSDP_Flood_pcap.pcap\n",
      "***************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 17:14:52,671 - INFO - Data/kitsune/output/ARP_MitM_pcap.tsv already exists!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_usage: 533.01 MB\n",
      "\n",
      "Start Data/kitsune/ARP_MitM_pcap.pcapng\n",
      "***************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 17:19:58,648 - INFO - Data/kitsune/output/SSL_Renegotiation_pcap.tsv already exists!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Data/kitsune/SSL_Renegotiation_pcap.pcap\n",
      "***************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 17:24:22,760 - INFO - Data/kitsune/output/Mirai_pcap.tsv already exists!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Data/kitsune/Mirai_pcap.pcap\n",
      "***************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 17:26:59,717 - INFO - Data/kitsune/output/SYN_DoS_pcap.tsv already exists!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Data/kitsune/SYN_DoS_pcap.pcap\n",
      "***************\n",
      "2531.4536652565002\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args = parser()\n",
    "    directory = args.directory\n",
    "    recursive = args.recursive\n",
    "    debug = args.debug\n",
    "\n",
    "    # Get subdirectories based on recursive flag\n",
    "    subdirectories = get_subdirectories(directory, recursive)\n",
    "\n",
    "    for subdir in subdirectories:\n",
    "        pcap_files = glob.glob(os.path.join(subdir, '*.pcap*'))\n",
    "        outdir = os.path.join(subdir, 'output')\n",
    "        \n",
    "        # If no PCAP files are found, check for TSV files as a fallback\n",
    "        if not pcap_files:\n",
    "            pcap_files = glob.glob(os.path.join(subdir, '*.tsv'))\n",
    "            if not pcap_files:\n",
    "                print(f\"Directory '{subdir}' has no files!\\n\")\n",
    "                continue\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "            print(f\"Directory '{outdir}' created.\")\n",
    "        \n",
    "        print(f\"Starting analysis for directory '{subdir}'\")\n",
    "        \n",
    "        # Process each file based on debug flag\n",
    "        process_pcap_files(pcap_files, outdir, debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nids",
   "language": "python",
   "name": "nids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
